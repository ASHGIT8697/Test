import os
import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

def scrape_data(input_file_path, output_file_path):
    # Initialize WebDriver
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # Run in headless mode for better performance
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    wait = WebDriverWait(driver, 10)

    # Read inputs from text file
    with open(input_file_path, 'r') as file:
        inputs = file.read().splitlines()

    # Prepare to write results to CSV
    with open(output_file_path, mode='w', newline='', encoding='utf-8') as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(["Input", "Scraped Data"])  # Change headers based on your table structure

        for input_value in inputs:
            try:
                # Construct URL
                base_url = "https://example.com/path/"  # Replace with actual base URL
                full_url = f"{base_url}{input_value}"

                # Open URL
                driver.get(full_url)

                # Wait for the table to load
                try:
                    table = wait.until(EC.presence_of_element_located((By.TAG_NAME, "table")))
                    rows = table.find_elements(By.TAG_NAME, "tr")
                    
                    # Extract data from rows and cells
                    for row in rows:
                        cells = row.find_elements(By.TAG_NAME, "td")
                        row_data = [cell.text.strip() for cell in cells]
                        csv_writer.writerow([input_value, *row_data])  # Save input and row data

                except TimeoutException:
                    print(f"Table not found or failed to load for URL: {full_url}")
                    continue

            except WebDriverException as e:
                print(f"Error accessing {full_url}: {e}")
                continue

    # Close the WebDriver
    driver.quit()

# Main execution
if __name__ == "__main__":
    input_file_path = input("Enter the path of the input text file: ").strip()
    output_file_path = input("Enter the path to save the output CSV file: ").strip()

    # Ensure the input file exists
    if not os.path.exists(input_file_path):
        print("Input file does not exist. Please provide a valid path.")
    else:
        scrape_data(input_file_path, output_file_path)
        print(f"Scraping complete. Data saved to {output_file_path}")
