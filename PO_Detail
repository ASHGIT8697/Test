import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_data(input_file_path, output_csv_path):
    # Base URL (Modify as per your requirement)
    base_url = "https://example.com/path/"
    
    # Read inputs from the text file
    try:
        with open(input_file_path, 'r') as file:
            inputs = [line.strip() for line in file.readlines()]
    except FileNotFoundError:
        print("Input file not found. Please check the path and try again.")
        return
    
    # Initialize an empty DataFrame to store results
    all_data = pd.DataFrame()

    for inp in inputs:
        url = f"{base_url}{inp}"
        print(f"Processing: {url}")
        
        try:
            # Fetch the page
            response = requests.get(url, timeout=10)
            response.raise_for_status()  # Raise HTTPError for bad responses
            
            # Parse the HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Locate the table (Modify the selector as per the actual table structure)
            table = soup.find('table')
            if not table:
                print(f"No table found on the page: {url}")
                continue
            
            # Extract table headers
            headers = [th.text.strip() for th in table.find_all('th')]
            
            # Extract table rows
            rows = []
            for tr in table.find_all('tr'):
                cells = [td.text.strip() for td in tr.find_all('td')]
                if cells:  # Avoid empty rows
                    rows.append(cells)
            
            # Convert to a DataFrame and append to all_data
            if rows:
                df = pd.DataFrame(rows, columns=headers if headers else None)
                df['Source URL'] = url  # Add URL as a reference column
                all_data = pd.concat([all_data, df], ignore_index=True)
            else:
                print(f"No data found in the table on page: {url}")
                
        except requests.exceptions.RequestException as e:
            print(f"Failed to fetch the page {url}: {e}")
        except Exception as e:
            print(f"An error occurred while processing {url}: {e}")
    
    # Save the collected data to a CSV file
    if not all_data.empty:
        all_data.to_csv(output_csv_path, index=False)
        print(f"Data successfully saved to {output_csv_path}")
    else:
        print("No data collected. CSV file not created.")

if __name__ == "__main__":
    # Prompt for file paths
    input_file = input("Enter the path to the input text file: ").strip()
    output_file = input("Enter the path to save the output CSV file: ").strip()
    scrape_data(input_file, output_file)
