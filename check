import os
import csv
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager


def scrape_data(input_file_path, output_file_path):
    # Initialize WebDriver
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # Run in headless mode for better performance
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    wait = WebDriverWait(driver, 20)

    # Read inputs from text file
    with open(input_file_path, 'r') as file:
        inputs = file.read().splitlines()

        for input_value in inputs:
            try:
                # Construct URL
                base_url = "https://src-na.corp.amazon.com/procurability/?asin="  # Replace with actual base URL
                last_part="&iog=1"
                full_url = f"{base_url}{input_value}{last_part}"

                # Open URL
                driver.get(full_url)

                # Wait for the table to load
                wait = WebDriverWait(driver, 10)  # Wait up to 10 seconds for elements to appear
                
                elements=driver.find_elements(By.CLASS_NAME,"text-success")
                
                data=[]
                for element in elements:
                    data.append(element.text)

                #save data to csv
                df=pd.DataFrame(data,columns=["ASIN","Procurability"])
                df.to_csv()


            except TimeoutException:
                print(f"ASIN not found or failed to load for URL: {full_url}")
                continue

    # Close the WebDriver
    driver.quit()


# Main execution
if __name__ == "__main__":
    input_file_path = input("Enter the path of the input text file: ").strip()
    output_file_path = input("Enter the path to save the output CSV file: ").strip()

    # Ensure the input file exists
    if not os.path.exists(input_file_path):
        print("Input file does not exist. Please provide a valid path.")
    else:
        scrape_data(input_file_path, output_file_path)
        print(f"Scraping complete. Data saved to {output_file_path}")
